\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{amsmath, amssymb}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{subcaption}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}


% Make parindent 0
\setlength{\parindent}{0pt}
\setlist{itemsep=0.5em}
\setlength{\parskip}{0.5em}
 
\geometry{margin=1in}
\pagestyle{fancy}
\fancyhf{}
\lhead{\textbf{Solutions}} % CHANGE NAME HERE
\rhead{\textbf{Data Analysis and Regression, Homework 2}}
\cfoot{\thepage}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\newcommand{\problem}[1]{\section*{Problem #1}}
\newcommand{\subproblem}[1]{\subsection*{Problem #1}}
\newcommand{\answer}{\textbf{\textit{\textcolor{red}{Answer:}}}\\}

\begin{document}

\section*{Instructions}
\begin{itemize}
    \item Homework 2 is due December 7th at 16:00 Chicago Time.
    \begin{itemize}
        \item We will not accept any submissions past 16:00:00, even if they are only one second late.
    \end{itemize}
    \item You \textbf{must} upload the following files to the class Canvas:
    \begin{itemize}
        \item \texttt{LASTNAME\_FIRSTNAME.pdf}
        \item \texttt{LASTNAME\_FIRSTNAME.ipynb}
    \end{itemize}
    \item Your code notebook \textbf{must} be runnable using my environment outlines in class 1 (Python 3.14, and the \texttt{requirements.txt}).
    \item You \textbf{must} use this template file and fill out your solutions for the written portion.
    \item Please note that your last name and first name should match what you appear on Canvas as.
    \item Include code snippets where required, as well as math and equations.
    \item Be \textit{concise} where possible, all of the homework probelms can be answered in a few lines of math, code, and words.
\end{itemize}
\hrule
\vspace{0.5cm}

\pagebreak

\problem{1: Hands-On OLS}

\subproblem{1.1: Setup}

Set your random seet using \texttt{np.random.seed(1)}. Generate $n=30$ observations where:
\begin{itemize}
    \item The predictor $X$ is drawn from a standard normal distribution, $X \sim N(0, 1)$.
    \item The error term $\epsilon$ is drawn from a normal distribution with mean 0 and standard deviation 1, $\epsilon \sim N(0, 1)$.
    \item The response variable is generated by the true relationship: $Y = 5 + 2X + \epsilon$.
\end{itemize}

Display a scatter plot of the generated data points $(X_i, Y_i)$.

\answer
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/problem1_scatter.png}
    \caption{Scatter plot of generated data points $(X_i, Y_i)$.}
\end{figure}




\newpage
\subproblem{1.2: A First Fit}

Using the data generated above, fit an OLS model. You should report:
\begin{enumerate}
    \item $\hat{\beta}_0$ and $\hat{\beta}_1$ estimates.
    \item Your confidence intervals and p-values for both coefficients.
    \item The $R^2$ value of your fit.
\end{enumerate}

\answer
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &        Y         & \textbf{  R-squared:         } &     0.840   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.834   \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
               & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{const} &       5.0677  &        0.155     &    32.787  &         0.000        &        4.751    &        5.384     \\
\textbf{X}     &       1.8516  &        0.153     &    12.110  &         0.000        &        1.538    &        2.165     \\
\bottomrule
\end{tabular}
\end{center}

Notes: \newline
 [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


\subproblem{1.3: Interpretation}

How do your estimated coefficients and confidence intervals compare to the true parameters?

\answer
The estimated coefficients are very close to the true parameters. The confidence intervals for both coefficients include the true values of 5 (intercept) and 2 (slope), indicating that our estimates are accurate and reliable. The p-values for both coefficients are approximately 0, suggesting that we can reject the null hypothesis that the coefficients are equal to zero.


\newpage
\subproblem{1.4: An Influential Point}

Now, modify your dataset by overriding the last obvervation to be the point $(X_{30}, Y_{30}) = (4, -5)$.
Refit your OLS model to this modified dataset, and report:
\begin{enumerate}
    \item $\hat{\beta}_0$ and $\hat{\beta}_1$ estimates.
    \item Your confidence intervals and p-values for both coefficients.
    \item The $R^2$ value of your fit.
\end{enumerate}


\answer
\begin{center}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}    &        Y         & \textbf{  R-squared:         } &     0.027   \\
\textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &    -0.008   \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
               & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{const} &       4.5388  &        0.500     &     9.082  &         0.000        &        3.515    &        5.563     \\
\textbf{X}     &       0.3533  &        0.402     &     0.879  &         0.387        &       -0.470    &        1.177     \\
\bottomrule
\end{tabular}
\end{center}

\subproblem{1.5: Interpretation}

How do your estimated coefficients and confidence intervals compare to the true parameters?

\answer
The estimated slope coefficient has changed dramatically from approximately 1.85 to 0.35 due to the addition of the influential point.

The confidence interval for the slope now includes zero, indicating that we cannot reject the null hypothesis. The p-value indicates that the probability of observing such a slope under the null hypothesis is quite high (0.774).


\newpage
\subproblem{1.6: Cook's Distance}

Calculate Cook's distance for all observations in the modified dataset from part (d).
Plot the Cook's distance values, and highlight the 31st observation. 

\answer

\begin{figure}[!ht]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/problem1_cooks_distance.png}
        \caption{Cook's Distance for Each Observation}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/problem1_influence_plot.png}
        \caption{Influence Plot}
    \end{subfigure}
    \caption{Cook's Distance and Influence Plot for Modified Dataset}
\end{figure}



\subproblem{1.7: What if?}

Suppose instead that the influential point you just added was $(X_{30}, Y_{30}) = (0, -5)$.

What would you expect the \textit{leverage} of this point to be relative to the $(4, -5)$ point you added before?

\answer
Under $X = \begin{pmatrix}1 & x_1\end{pmatrix}^\top$ design matrix setup, the formula of leverage is $h_{ii} = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{j=1}^n (x_j - \bar{x})^2}$. So leverage is influenced by the distance of the predictor value from the mean of the predictor values. Since 0 is exactly the population mean of the X values, the leverage of the point (0, -5) would be lower than that of the point (4, -5).








\newpage
\problem{2: Central Limit Theorem}

We found in Class 2, that if $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$,
then as $n \to \infty$, then $\beta_{\text{OLS}} \to \mathcal{N}(\beta_{\text{OLS}}, \sigma_{\text{OLS}}^2)$, where $\sigma_{\text{OLS}}^2 = \sigma^2 / \sum_{i=1}^n (x_i - \bar{x})^2$.

The goal of this problem is for you to empirically verify that this 
results holds \textit{even if} $\epsilon_i$ are not normally distributed.

\subproblem{2.1: Setup}

First, define the true model as:

$$
y = 2x
$$

Where $x$ is sampled from the uniform distribution $x \sim \text{Uniform}(0, 1)$.
Note that we are not including an intercept, nor any noise.

Second, define the noise $\epsilon_i$ to be sampled from 2 different distributions:
\begin{itemize}
    \item A Bernoulli distribution with $p=0.5$, and values $\{-1, 1\}$ (the Radamacher distribution).
    \item A uniform distribution with $\epsilon_i \sim \text{Uniform}(-1, 1)$.
\end{itemize}

For $n=500$, please display histograms of the 2 noise distributions you defined above.

\answer

\begin{figure}[!ht]
    \centering
    \includegraphics[width = .6\textwidth]{figures/problem2_error_histograms.png}
\end{figure}


\newpage
\subproblem{2.2: Empirical Verification}

\textbf{Note:} You should not re-sample your $x$ values, only the $\epsilon_i$ values.
That is, you should have a fixed set of $x$ values for this entire problem.

For $n \in {10, 100, 1000}$, and for each noise distribution defined above, do the following:
\begin{enumerate}
    \item Sample $n$ values of $\epsilon_i$ from the noise distribution.
    \item Generate the target values as $y_i = 2x_i + \epsilon_i$.
    \item Fit an OLS regression to the data $(x_i, y_i)$, and obtain the estimate $\hat{\beta}_{\text{OLS}}$.
    \item Repeat (1)-(3) 1,000 times, and save all of the $\hat{\beta}_{\text{OLS}}$ estimates.
\end{enumerate}
You should now have 6 sets of 1,000 $\hat{\beta}_{\text{OLS}}$ estimates (2 noise distributions $\times$ 3 values of $n$).

Include your simulation code below:

\answer

\begin{lstlisting}[language=Python]
res_beta = {}
x= np.random.rand(1000)
for n in [10, 100, 1000]:
    x_temp = x[:n]
    temp = {'beta1': [], 'beta2': []}
    for i in range(1000):
        eps1 = 2 * np.random.binomial(1, 0.5, n) - 1
        eps2 = 2 * np.random.rand(n) - 1

        y1 = 2 * x_temp + eps1
        y2 = 2 * x_temp + eps2
        
        beta1 = sm.OLS(y1, x_temp).fit().params[-1]
        beta2 = sm.OLS(y2, x_temp).fit().params[-1]
        temp['beta1'].append(beta1)
        temp['beta2'].append(beta2)
    res_beta[n] = pd.DataFrame(temp)
\end{lstlisting}

\newpage
\subproblem{2.3: Visualization}

For each of the 6 sets of $\hat{\beta}_{\text{OLS}}$ estimates obtained above, plot a histogram of the estimates.
Additionally, conduct a Shapiro-Wilk test for normality on each set of estimates,
report your p-values, and briefly discuss your results.

\answer

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/problem2_beta_histograms.png}
    \caption{Histograms of $\hat{\beta}_{\text{OLS}}$ estimates for different sample sizes and noise distributions.}
\end{figure}


\begin{center}
\begin{tabular}{lrrrrrr}
\toprule
 & 10\_beta1 & 10\_beta2 & 100\_beta1 & 100\_beta2 & 1000\_beta1 & 1000\_beta2 \\
\midrule
shapiro\_stat & 0.994535 & 0.996154 & 0.999380 & 0.997112 & 0.998398 & 0.998662 \\
shapiro\_p & 0.001090 & 0.014110 & 0.989955 & 0.068768 & 0.489748 & 0.662977 \\
\bottomrule
\end{tabular}
\end{center}

The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. From the p-values reported above, with smaller sammple (n=10), it is less likely that the estimates are normally distributed. This shows the asymptotic normality through Central Limit Theorem.


\newpage
\problem{3: Weighted Least Squares}

This problem focuses on verifying the findings of Class 2 regarding
Weighted Least Squares (WLS).

\subproblem{3.1: Setup}

Similar to before, define the true model as:
$$
y = 2x
$$
Where $x$ is sampled from the uniform distribution $x \sim \text{Uniform}(1, 2)$.

Second, define the noise $\epsilon_i$ to be sampled from a normal distribution with mean 0,
and variance $\sigma_i^2$ that depends on $x_i$ as follows:
$$
\sigma_i^2 = \frac{1}{x_i^2}
$$

For $n=500$, please plot the generated data points $(x_i, y_i + \epsilon_i)$.

\answer
\begin{figure}[!ht]
    \centering
    \includegraphics[width = 0.6\textwidth]{figures/problem3_scatter.png}
\end{figure}


\newpage
\subproblem{3.2: Naive OLS}

Fit a standard OLS regression to the data generated above.

Report your estimate $\hat{\beta}_{\text{OLS}}$, and the confidence interval for the estimate.

\answer
\begin{center}
\begin{tabular}{lcccccc}
               & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{const} &       0.0723  &        0.169     &     0.428  &         0.669        &       -0.260    &        0.404     \\
\textbf{x1}    &       1.9801  &        0.110     &    17.937  &         0.000        &        1.763    &        2.197     \\
\bottomrule
\end{tabular}
\end{center}

\subproblem{3.3: Interpretation}

Briefly discuss whether naive OLS will be under or over confident in its estimate of $\hat{\beta}_{\text{OLS}}$,
and why.

\answer
\paragraph{Case 1}
Understanding this Problem 3 as having measurement error in X, this relates to the attenuation bias (larger variance of x, so $\beta$ is biased towards zero). But this is not about the heteroscedasticity, so WLS does not help to solve this bias issue.

\paragraph{Case 2}
When heteroscedasticity exists, naive OLS beta has larger variance compared to WLS. This is because WLS is the efficient estimator under heteroscedasticity.

(Proof sketch) Let $\Var(\varepsilon \mid X) = \Omega$ where \(\Omega = \mathrm{diag}(\sigma_1^2,\dots,\sigma_n^2) \succ 0\). Then

\[
\begin{aligned}
\hat\beta_{\mathrm{OLS}} &= (X^\top X)^{-1} X^\top y 
&& \Var(\hat\beta_{\mathrm{OLS}} \mid X)
= (X^\top X)^{-1} X^\top \Omega X (X^\top X)^{-1}\\
\hat\beta_{\mathrm{WLS}}
&= (X^\top \Omega^{-1} X)^{-1} X^\top \Omega^{-1} y
&&\Var(\hat\beta_{\mathrm{WLS}} \mid X)
= (X^\top \Omega^{-1} X)^{-1}
\end{aligned}
\]

Let \(A_{\mathrm{OLS}} = (X^\top X)^{-1}X^\top\) and
\(A_{\mathrm{WLS}} = (X^\top \Omega^{-1} X)^{-1} X^\top \Omega^{-1}\), which satisfy \(AX = I\). Then you can derive that
$$
\operatorname{Var}(\hat{\beta}_{\mathrm{OLS}} \mid X)=\operatorname{Var}(\hat{\beta}_{\mathrm{WLS}} \mid X)+(A_{\mathrm{OLS}}-A_{\mathrm{WLS}}) \Omega(A_{\mathrm{OLS}}-A_{\mathrm{WLS}})^{\top}$$
Then $\Omega \succ 0$ implies
$$
\Var(\hat\beta_{\mathrm{OLS}} \mid X) -\Var(\hat\beta_{\mathrm{WLS}} \mid X) \succeq 0
$$



% \newpage
% \subproblem{3.4: Visualization}

% Plot a Q-Q plot of the residuals from the naive OLS regression. What do you observe?

% \answer
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/problem3_OLS_qq_plot.png}
%     \caption{Q-Q Plot of Residuals from Naive OLS Regression}
% \end{figure}

% Fromt he Q-Q plot, we can see that the residuals deviate from the normal distribution, especially in the tails.


\subproblem{3.5: Weighted Least Squares}

Fit a Weighted Least Squares regression to the data generated above, using weights $w_i = x_i^2$.
Report your estimate $\hat{\beta}_{\text{WLS}}$, and the confidence interval for the estimate.

\answer
\begin{center}
\begin{tabular}{lcccccc}
               & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{const} &       0.1693  &        0.180     &     0.939  &         0.348        &       -0.185    &        0.523     \\
\textbf{x1}    &       1.9187  &        0.110     &    17.393  &         0.000        &        1.702    &        2.135     \\
\bottomrule
\end{tabular}
\end{center}


\subproblem{3.4: Visualization}
Plot a Q-Q plot of the residuals from the naive OLS regression. What do you observe?

\subproblem{3.6: Visualization}

Plot a Q-Q plot of the weighted residuals from the WLS regression. What do you observe compared
to the Q-Q plot from the naive OLS regression?

% \answer
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/problem3_WLS_qq_plot.png}
%     \caption{Q-Q Plot of Weighted Residuals from WLS Regression}
% \end{figure}

\begin{figure}[!ht]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/problem3_OLS_qq_plot.png}
        \caption{Naive OLS}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/problem3_WLS_qq_plot.png}
        \caption{WLS}
    \end{subfigure}
    \caption{Comparison of Q-Q Plots: Naive OLS vs WLS}
\end{figure}

From the OLS Q-Q plot, we can see that the residuals slightly deviate from the normal distribution, especially in the tails.
It is improved in the WLS Q-Q plot, however, there are still some deviations from normality, indicating that while WLS has improved the fit, it may not fully address all issues related to the residuals' distribution.






\end{document}